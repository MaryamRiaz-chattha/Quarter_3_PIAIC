{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUD2RP48zgawa5bm4EFfVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryamRiaz-chattha/Quarter_3_PIAIC/blob/main/Project_1_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY_3')\n",
        "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY"
      ],
      "metadata": {
        "id": "l9TnlkSqKMQ_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain_google_genai"
      ],
      "metadata": {
        "id": "8Z9hEL0ZQxA-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define the prompt templates\n",
        "prompts = {\n",
        "    \"first\": PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "        template=\"\"\"\n",
        "Provide a detailed explanation of the topic: \"{text}\".\n",
        "\n",
        "The explanation should:\n",
        "- Be clear and easy to understand for a general audience.\n",
        "- Include relevant examples or applications, if possible.\n",
        "- Avoid unnecessary jargon while maintaining accuracy.\n",
        "\"\"\"\n",
        "    ),\n",
        "    \"second\": PromptTemplate(\n",
        "        input_variables=[\"first_response\"],\n",
        "        template=\"\"\"\n",
        "Based on the following response: \"{first_response}\", create a detailed markdown explanation.\n",
        "\n",
        "The markdown explanation should include:\n",
        "# Introduction\n",
        "A brief overview of the topic.\n",
        "## Key Concepts\n",
        "Main ideas or points related to the topic.\n",
        "\n",
        "## In-depth Analysis\n",
        "A deeper exploration of critical subtopics or concepts.\n",
        "\n",
        "## Examples or Applications\n",
        "Relevant real-world examples, applications, or scenarios.\n",
        "\n",
        "## Conclusion\n",
        "A summary that reinforces the key takeaways.\n",
        "\"\"\"\n",
        "    ),\n",
        "    \"third\": PromptTemplate(\n",
        "        input_variables=[\"second_response\"],\n",
        "        template=\"\"\"\n",
        "Convert the following detailed explanation into a well-structured image generation prompt:\n",
        "{second_response}\n",
        "\n",
        "The image generation prompt should:\n",
        "- Be specific, vivid, and descriptive.\n",
        "- Highlight key visual elements, styles, or themes.\n",
        "- Provide sufficient details to create a realistic or artistic image.\n",
        "- Avoid ambiguity to ensure accurate image generation.\n",
        "\"\"\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Configure the ChatGoogleGenerativeAI model with different settings\n",
        "llm_configurations = {\n",
        "    \"first\": ChatGoogleGenerativeAI(\n",
        "        api_key=GEMINI_API_KEY,\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        temperature=0.6\n",
        "    ),\n",
        "    \"second\": ChatGoogleGenerativeAI(\n",
        "        api_key=GEMINI_API_KEY,\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        temperature=0.1,\n",
        "        max_output_tokens=1000\n",
        "    ),\n",
        "    \"third\": ChatGoogleGenerativeAI(\n",
        "        api_key=GEMINI_API_KEY,\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        temperature=0.5,\n",
        "        max_output_tokens=300\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Create the chains\n",
        "chains = {\n",
        "    \"first\": prompts[\"first\"] | llm_configurations[\"first\"],\n",
        "    \"second\": prompts[\"second\"] | llm_configurations[\"second\"],\n",
        "    \"third\": prompts[\"third\"] | llm_configurations[\"third\"],\n",
        "}\n",
        "\n",
        "# Input text for the chain\n",
        "text = \"langchain uses\"\n",
        "\n",
        "# Invoke the chains\n",
        "first_response = chains[\"first\"].invoke({\"text\": text})\n",
        "print(\"\\n\\n\\nFirst chain:\\n\\n\")\n",
        "display(Markdown(first_response.content))\n",
        "\n",
        "second_response = chains[\"second\"].invoke({\"first_response\": first_response.content})\n",
        "print(\"\\n\\n\\nSecond chain:\\n\\n\")\n",
        "display(Markdown(second_response.content))\n",
        "\n",
        "third_response = chains[\"third\"].invoke({\"second_response\": second_response.content})\n",
        "print(\"\\n\\n\\nThird chain:\\n\\n\")\n",
        "display(Markdown(third_response.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZJA2L67ISRUD",
        "outputId": "c377d54d-8504-4740-d1e5-7f15b97c6e05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "First chain:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down what \"LangChain uses\" means in a way that's easy to grasp.\n\n**What is LangChain?**\n\nImagine you have a super-smart AI, like the ones that power chatbots or write text. These AIs, often called Large Language Models (LLMs), are incredibly powerful but they can't do everything on their own. They need tools and a way to connect with the outside world to be truly useful.\n\nThat's where LangChain comes in. Think of LangChain as a **framework** or a **toolkit** that helps developers build applications that use and extend the capabilities of these LLMs. It's like providing the AI with a set of instructions, tools, and a way to interact with other systems.\n\n**So, what does \"LangChain uses\" actually mean?**\n\nWhen we say \"LangChain uses,\" we're talking about the different ways this toolkit is employed to enhance and expand the functionality of LLMs. Here are the core concepts and how they're used:\n\n1. **Chaining LLMs Together (Chains):**\n\n   - **The Idea:** Instead of just asking an LLM one question, you can create a sequence of actions. The output of one LLM can become the input for another.\n   - **Example:** Let's say you want to write a blog post about a specific topic. You could use a chain like this:\n      - **LLM 1:** Asks the user for the topic and generates an outline.\n      - **LLM 2:** Takes the outline and writes the first draft of the blog post.\n      - **LLM 3:** Reviews the draft for grammar and style.\n   - **Why it's useful:** This allows you to break down complex tasks into smaller, more manageable steps and leverage the strengths of different LLMs.\n\n2. **Connecting to Data Sources (Data Connections):**\n\n   - **The Idea:**  LLMs are trained on vast amounts of data, but they don't know about your specific information (like your company's internal documents or the latest news). LangChain helps LLMs access and use this external data.\n   - **Example:**\n      - You could build a customer support chatbot that can access your company's knowledge base and product manuals.\n      - You could create a research tool that can search the web for relevant articles and summarize them.\n   - **Why it's useful:** This allows LLMs to provide more accurate and relevant answers, grounding their responses in factual data.\n\n3. **Using Tools (Tools & Agents):**\n\n   - **The Idea:**  LLMs can be given access to \"tools\" to perform specific actions, like searching the internet, doing calculations, or using APIs. An \"agent\" is a system that decides which tool to use based on the user's request.\n   - **Example:**\n      - You could build a chatbot that can book flights by using an airline booking API.\n      - You could create an AI assistant that can summarize a web page, translate it into another language, and then email it to you.\n   - **Why it's useful:** This allows LLMs to go beyond just generating text; they can actually perform actions and interact with the real world.\n\n4. **Memory:**\n\n    - **The Idea:** LLMs are typically \"stateless,\" meaning they don't remember past conversations. LangChain provides ways to give LLMs memory, so they can track the context of a conversation and provide more relevant responses.\n    - **Example:** A chatbot that can remember what you've talked about earlier in the conversation and refer back to it.\n    - **Why it's useful:** This creates more natural and engaging conversations with LLMs, making them more human-like.\n\n5. **Prompt Management:**\n\n   - **The Idea:**  The way you phrase your questions (prompts) significantly impacts the quality of the LLM's response. LangChain provides tools for managing and optimizing prompts.\n   - **Example:** You can use prompt templates to create consistent and effective prompts for different types of tasks.\n   - **Why it's useful:** This helps you get the best possible results from LLMs by using carefully crafted prompts.\n\n**In Simple Terms:**\n\nImagine an LLM is like a really smart student but needs the right books, tools, and guidance to do their best work. LangChain is like the school library, the workshop, and the teacher, all rolled into one. It provides the resources and structure needed to build powerful applications with LLMs.\n\n**Why is LangChain Important?**\n\n- **Democratizes AI:** It makes it easier for developers (and even non-developers) to build complex AI applications without needing deep expertise in AI.\n- **Faster Development:** It provides pre-built components and tools, speeding up the development process.\n- **More Powerful Applications:** It enables the creation of AI applications that are more capable, context-aware, and useful.\n\n**In Conclusion:**\n\n\"LangChain uses\" refers to the various ways this framework is employed to extend the capabilities of Large Language Models. It's about chaining LLMs, connecting to data sources, using tools, adding memory, and managing prompts, all to create more powerful and practical AI applications. Essentially, LangChain is a powerful tool that helps developers harness the full potential of LLMs.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Second chain:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Understanding LangChain: A Framework for Enhancing Large Language Models\n\n## Introduction\n\nThis document provides a detailed explanation of LangChain, a powerful framework designed to extend the capabilities of Large Language Models (LLMs). LLMs, while incredibly intelligent, often require additional tools and connections to be truly effective in real-world applications. LangChain acts as a bridge, providing developers with the necessary resources to build sophisticated AI applications. This explanation will break down what \"LangChain uses\" means, exploring its core functionalities and their significance.\n\n## Key Concepts\n\nLangChain is essentially a toolkit that empowers developers to build applications leveraging the power of LLMs. It achieves this through several key concepts:\n\n*   **Chaining LLMs:** Creating sequences of actions where the output of one LLM becomes the input for another.\n*   **Data Connections:** Enabling LLMs to access and utilize external data sources beyond their training data.\n*   **Tools & Agents:** Providing LLMs with access to specific tools and the ability to decide which tool to use based on user requests.\n*   **Memory:** Giving LLMs the ability to remember past interactions and maintain context within a conversation.\n*   **Prompt Management:** Offering tools to manage and optimize the prompts used to interact with LLMs.\n\n## In-depth Analysis\n\nLet's delve deeper into each of these core concepts:\n\n### 1. Chaining LLMs Together (Chains)\n\n*   **Concept:** Instead of relying on a single LLM for a complex task, LangChain allows you to create a chain of LLMs. Each LLM in the chain can handle a specific part of the task, leveraging its unique strengths.\n*   **Mechanism:** The output of one LLM is passed as input to the next, creating a workflow.\n*   **Benefits:** This approach allows for the decomposition of complex tasks into smaller, more manageable steps. It also enables the use of different LLMs for different parts of the process, optimizing performance.\n*   **Example:** A blog post creation chain might involve one LLM generating an outline, another writing the draft, and a third reviewing the draft for grammar and style.\n\n### 2. Connecting to Data Sources (Data Connections)\n\n*   **Concept:** LLMs are trained on vast datasets, but they lack access to specific, real-time, or proprietary information. LangChain facilitates the connection of LLMs to external data sources.\n*   **Mechanism:** LangChain provides tools to access and retrieve data from various sources, such as databases, APIs, and web pages.\n*   **Benefits:** This allows LLMs to provide more accurate, relevant, and context-aware responses, grounding their answers in factual data.\n*   **Example:** A customer support chatbot can access a company's knowledge base to answer customer queries accurately.\n\n### 3. Using Tools (Tools & Agents)\n\n*   **Concept:** LangChain enables LLMs to interact with the real world by providing them with access to tools. An \"agent\" is a system that decides which tool to use based on the user's request.\n*   **Mechanism:** Tools can be anything from search engines and calculators to APIs for booking flights or sending emails. The agent uses the LLM's reasoning capabilities to determine the appropriate tool for a given task.\n*   **Benefits:** This allows LLMs to perform actions beyond just generating text, making them more versatile and practical.\n*   **Example:** An AI assistant can book a flight by using an airline booking API or summarize a web page using a web scraping tool.\n\n### 4. Memory\n\n*   **Concept:** LLMs are typically stateless, meaning they don't remember past interactions. LangChain provides mechanisms to give LLMs memory, allowing them to track the context of a conversation.\n*   **Mechanism:** LangChain offers various memory implementations, such as storing conversation history or using a knowledge graph.\n*   **Benefits:** This creates more natural and engaging conversations with LLMs, making them more human-like and contextually aware.\n*   **Example:** A chatbot can remember what you've talked about earlier in the conversation and refer back to it.\n\n### 5. Prompt Management\n\n*   **Concept:** The way you phrase your questions (prompts) significantly impacts the quality of the LLM's response. LangChain provides tools for managing and optimizing prompts.\n*   **Mechanism:** LangChain offers prompt templates, prompt chaining, and other techniques to create consistent and effective prompts.\n*   **Benefits:** This helps you get the best possible results from LLMs by using carefully crafted prompts, reducing ambiguity and improving accuracy.\n*   **Example:** Using prompt templates to ensure consistent formatting and instructions for different types of tasks.\n\n## Examples or Applications\n\nLangChain's capabilities enable a wide range"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Third chain:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a well-structured image generation prompt based on the provided text, aiming for clarity and visual appeal:\n\n**Image Generation Prompt:**\n\n**Subject:** A stylized, interconnected network diagram representing LangChain's core functionalities, set against a futuristic, tech-inspired background.\n\n**Visual Elements:**\n\n*   **Central Hub:** A large, glowing, abstract brain icon at the center, symbolizing the Large Language Model (LLM). This brain should have a network of glowing lines emanating from it.\n*   **Chains (Chaining LLMs):**  Several interconnected, flowing lines of light, like a chain, moving from one smaller brain icon to another. These lines should have a directional flow, indicating the passing of information.\n*   **Data Connections:** A series of data icons (database cylinders, API connectors, web page symbols) connected to the main brain with glowing data streams. These should look like data flowing in and out.\n*   **Tools & Agents:**  Small icons representing tools (a magnifying glass for search, a calculator, a calendar, an email symbol) orbiting the central brain, with a small, robotic agent icon (a stylized robot head) directing the flow to these tools with thin beams of light.\n*   **Memory:** A circular, pulsating waveform or a memory chip icon connected to the central brain, with a faint trail of light representing past interactions.\n*   **Prompt Management:** A stylized speech bubble"
          },
          "metadata": {}
        }
      ]
    }
  ]
}